INVENTORS BUILD FOR SDG SCREENING PHASE
EXCEL
QUESTION 1
· How do you use Excel to clean and transform messy data, including techniques like filtering, sorting, and text manipulation?
Excel is a powerful tool for cleaning and transforming messy data. Hera are some techniques you can use, including filtering, sorting, and text manipulation:
1. Filtering: Excel allows you to filter data based on specific criteria. To do this, select your data range and go to the “Data” tab. Click on the “ Filter” button, and you will see filter arrows appear in each column header. Click on the filter arow of the column you want to filter and select the criteria you want to apply. This helps you focus on specific data and remove unnecessary rows.
2. Sorting: Sorting helps you organize it in a desired order. Select the data range and go to the “Data”  tab. Click on the “sort” button, and a dialog box will appear. Choose the column you want to sort by and specify whether you want it in ascending or descending order. You can also sort by multiple columns by specifying the sort priority.
3. Text Manipulation: Excel provides several functions for manipulating text data. Here are a few examples
· CONCATENATE: Combines text from different cells into a single cell. For example, if you have first names in column A and last names in column B, you can use the formula =CONCATENATE(A1, “ “, B1) to merge them into one cell.
· LEFT/RIGHT/MID: Extracts a specific number of characters from the left, right, or middle of a text string. For example, if you have a column with full names and want to extract only the first names , you can use the formula =LEFT(A1, FIND(“ “, A1)-1)
· SUBSTITUTE: Replaces specific text within a cell with new text. For example, If you have a column with messy data and want to replace all occurences of “USA” with United States,” you can use the formula =SUBSTITUTE(A1, “USA”, “UNITED STATES”)
QUESTION 2
· Can you explain the concept of data validation in Excel and describe some techniques for enforcing data integrity and preventing errors?
Data validation in Excel refers to the process of setting up rules and constraints to ensure that data entered into a cell meets specific criteria or conditions. It helps enforce data integrity and prevent errors by restricting the type and range of data that can be entered.
Here are some techniques for enforcing data integrity and preventing errors using data validation in Excel:
1. Data Type Validation: You can specify the type of data that can be entered in a cell, such as numbers, dates, text, or time. This ensures that the entered data matches the expected format.
2. Range Validation: You can set a range of acceptable values for a cell. For example, you can define a range of valid dates, a list of allowed choices, or a minimum and maximum numeric value. This helps to limit data entry errors and ensures that the entered data falls within the desired range.
3. Length and Character Limit Validation: You can restrict the length of text or the number of characters that can be entered in a cell. This is useful when you have specific requirements for the length of data, such as limiting a text field to a certain number of characters.
4. Formula-Based Validation: You can use formulas to define custom validation rules. This allows you to create complex conditions based on cell references and functions. For example, you can create a formula that checks if the entered value is greater than the value in another cell.
5. Input Message and Error Alert: You can provide users with input messages that provide instructions or guidelines for data entry. Additionally, you can set up error alerts to notify users when invalid data is entered, displaying a customized error message.
6. Drop-down Lists: You can create drop-down lists using data validation to provide users with predefined choices. This helps to ensure that only valid options are selected and reduces the chances of incorrect or inconsistent data entry.
7. Data Validation across Multiple Cells: You can apply data validation rules to multiple cells at once. This is useful when you want to maintain consistency across a range of related cells, ensuring that data entered in one cell meets the validation criteria in another.
QUESTION 3
· How would you use Excel to build and analyze pivot tables and charts, including techniques for summarizing and aggregating data?

1. Prepare your data: Organize your data in a tabular format with columns representing different variables and rows containing data entries. Ensure that each column has a header.
2. Select the data range: Highlight the entire range of data you want to analyze, including the column headers.
3. Insert a pivot table: Go to the "Insert" tab in Excel's ribbon, click on "PivotTable," and choose "PivotTable" from the dropdown menu.
4. Specify the data source: In the "Create PivotTable" dialog box, ensure that the selected range matches the data you want to analyze. You can choose to place the pivot table in a new worksheet or an existing one.
5. Design the pivot table: The PivotTable Field List will appear on the right side of the Excel window. Drag and drop the field names from your data into the "Rows" and "Values" areas Of the PivotTable Field List. The "Rows" area determines the rows of the pivot table, while the "Values" area specifies the data to be summarized.
6. Aggregate and summarize data: By default, Excel provides a sum of numeric data. However, you can change the summary function by clicking on the drop-down arrow beside the field name in the "Values" area and selecting a different aggregation function like average, count, min, max, etc. You can also apply multiple aggregation functions to different fields.
7. Add filters and columns: You can add additional field names to the "Columns" and "Filters" areas of the PivotTable Field List. These allow you to further segment and filter your data in the pivot table.
8. Format and customize the pivot table: Excel offers various formatting options to customize the appearance of your pivot table. You can change the layout, apply styles, add subtotals, and modify the pivot table options by right-clicking within the pivot table and accessing the PivotTable Options.
9. Create pivot charts: With the pivot table selected, go to the "Insert" tab and choose the desired chart type from the "Charts" group. Excel will automatically create a chart based on the pivot table data.
10. Customize the pivot chart: You can modify the chart's appearance and formatting by selecting various chart elements and using the formatting options available in the "Chart Design" and "Chart Format" tabs of Excel's ribbon.
11. Analyze and explore data: Once the pivot table and chart are set up, you can easily analyze and explore the data by filtering, sorting, and drilling down into specific details. You can also update the pivot table and chart by refreshing the data if your underlying dataset changes.
By following these steps, you can leverage Excel's pivot tables and charts to summarize and analyze data effectively, providing valuable insights and visual representations of your data.


SQL
QUESTION 1
· Can you explain the basic concepts of relational databases and SQL and describe how they are used to store and retrieve data?
Relational databases and SQL (Structured Query Language) are closely connected and are used to store and retrieve data in a structured manner. Here's an explanation of the basic concepts:

Relational Databases:
A relational database is a type of database that organizes data into tables, which consist of rows and columns. Each table represents a specific entity or concept, and the relationships between tables are established based on common data elements. The relational model provides a systematic way to store and manage structured data.

Key Concepts in Relational Databases:

1. Tables: A table is a collection of related data organized into rows (records) and columns (attributes). Each row represents a specific instance or entry, while each column represents a data attribute.
2. Primary Key: A primary key is a unique identifier for each row in a table. It ensures that each row is uniquely identifiable and helps establish relationships with other tables.
3. Foreign Key: A foreign key is a field in one table that refers to the primary key of another table. It establishes a relationship between the two tables, enabling data integrity and consistency.
4. Relationships: Relationships define how tables are connected to each other. The most common relationships are one-to-one, one-to-many, and many-to-many. They provide a way to link related data between tables.

Structured Query Language (SQL):
SQL is a programming language designed for managing and manipulating data in relational databases. It provides a standardized way to interact with the database, allowing users to create, retrieve, update, and delete data.

Key Concepts in SQL:

1. Data Manipulation Language (DML): SQL includes commands for retrieving and modifying data within the database. Common DML commands include SELECT (retrieve data), INSERT (insert new data), UPDATE (modify existing data), and DELETE (remove data).
2. Data Definition Language (DDL): DDL commands are used to define and modify the structure of the database. They include commands such as CREATE TABLE (create a new table), ALTER TABLE (modify existing table structure), and DROP TABLE (remove a table).
3. Querying and Filtering: SQL allows you to query the database using the SELECT statement, which allows you to specify criteria for filtering and sorting data. You can use conditions, functions, and logical operators to refine your queries.
4. Joining Tables: SQL supports joining multiple tables to retrieve data from related tables. You can use various types of joins, such as INNER JOIN, LEFT JOIN, and RIGHT JOIN, to combine data based on common keys.
5. Aggregation Functions: SQL provides aggregate functions like SUM, COUNT, AVG, MIN, and MAX to calculate summary statistics from data. These functions enable data aggregation and analysis.
6. Indexing: SQL allows the creation of indexes on columns to enhance data retrieval performance. Indexes help to speed up searches and improve query efficiency.
7. By utilizing SQL queries and commands, users can interact with relational databases, retrieve specific data based on criteria, perform complex data analysis, modify database structure, and maintain data integrity through relationships and constraints. SQL is a powerful language for managing and manipulating data stored in relational databases.


QUESTION 2
· How do you use SQL to create and modify database objects like tables, views, indexes, and stored procedures?
In SQL, you can use Data Definition Language (DDL) statements to create and modify database objects such as tables, views, indexes, and stored procedures. Here's an overview of how to use SQL to work with these objects:

Creating Tables:
To create a table, you use the CREATE TABLE statement. Here's an example:
CREATE TABLE Employees (
  ID INT PRIMARY KEY,
  Name VARCHAR(100),
  Age INT,
  Department VARCHAR(50)
);
This statement creates a table named "Employees" with columns ID, Name, Age, and Department.

Modifying Tables:
To modify an existing table, you use the ALTER TABLE statement. Here are some common modifications:

Adding a Column:
ALTER TABLE Employees
ADD Salary DECIMAL(10,2);
This statement adds a new column "Salary" to the "Employees" table.

Modifying a Column:
ALTER TABLE Employees
ALTER COLUMN Age INT NOT NULL;
This statement modifies the "Age" column, making it not nullable.

Dropping a Column:
ALTER TABLE Employees
DROP COLUMN Department;
This statement removes the "Department" column from the "Employees" table.

Creating Views:
Views are virtual tables that are derived from one or more tables. They can simplify complex queries or provide a customized view of data. To create a view, you use the CREATE VIEW statement. Here's an example:
CREATE VIEW EmployeeSummary AS
SELECT Name, Age, Department
FROM Employees
WHERE Age > 30;
This statement creates a view named "EmployeeSummary" that selects specific columns from the "Employees" table based on a condition.

Creating Indexes:
Indexes improve the performance of queries by speeding up data retrieval. To create an index, you use the CREATE INDEX statement. Here's an example:
CREATE INDEX idx_EmployeeName ON Employees (Name);
This statement creates an index named "idx_EmployeeName" on the "Name" column of the "Employees" table.

Creating Stored Procedures:
Stored procedures are sets of SQL statements that are stored and executed on the database server. They provide a way to encapsulate logic and perform complex operations. To create a stored procedure, you use the CREATE PROCEDURE statement. Here's an example:
CREATE PROCEDURE GetEmployeeByID
  @EmployeeID INT
AS
BEGIN
  SELECT * FROM Employees WHERE ID = @EmployeeID;
END;
This statement creates a stored procedure named "GetEmployeeByID" that accepts an input parameter "@EmployeeID" and retrieves the corresponding employee from the "Employees" table.

These examples demonstrate the basic syntax for creating and modifying database objects using SQL. Keep in mind that the specific syntax and features may vary depending on the database management system you are using.
QUESTION 3:
· How do you write basic SQL queries to extract data from a database, including techniques like filtering, sorting, and grouping data?
To extract data from a database using SQL, you can write SELECT statements with various techniques such as filtering, sorting, and grouping. Here's an overview of how to write basic SQL queries for these operations:

Retrieving All Columns and Rows:
To retrieve all columns and rows from a table, you use the SELECT statement without any additional clauses. For example:
SELECT * FROM TableName;
Replace "TableName" with the name of the table from which you want to retrieve data.

Filtering Data:
To retrieve specific rows based on certain conditions, you can use the WHERE clause in the SELECT statement. For example:
SELECT * FROM TableName WHERE Condition;
Replace "TableName" with the table name, and "Condition" with a logical expression that specifies the filter criteria. Here's an example that retrieves employees with ages greater than 30:
SELECT * FROM Employees WHERE Age > 30;

Sorting Data:
To sort the retrieved data in a specific order, you can use the ORDER BY clause in the SELECT statement. For example:
SELECT * FROM TableName ORDER BY ColumnName [ASC|DESC];
Replace "TableName" with the table name, "ColumnName" with the column you want to sort by, and optionally specify ASC (ascending) or DESC (descending) to control the sort order. Here's an example that retrieves employees sorted by their names in ascending order:
SELECT * FROM Employees ORDER BY Name ASC;

Grouping Data:
To group data based on specific columns and perform aggregate functions on grouped data, you can use the GROUP BY clause in the SELECT statement. For example:
SELECT Column1, Column2, AggregateFunction(Column3)
FROM TableName
GROUP BY Column1, Column2;
Replace "TableName" with the table name, "Column1" and "Column2" with the columns you want to group by, and "AggregateFunction" with a function like SUM, COUNT, AVG, MIN, or MAX to perform calculations on grouped data. Here's an example that calculates the total salary for each department:
SELECT Department, SUM(Salary) AS TotalSalary
FROM Employees
GROUP BY Department;

These are basic examples of SQL queries for extracting data from a database. You can combine multiple techniques, use complex conditions, and apply other SQL clauses to perform more advanced data retrieval and analysis operations.





PYTHON
QUESTION 1
· How do you use Python libraries like NumPy and Pandas to preprocess and manipulate data for machine learning applications?
NumPy:
NumPy is a fundamental library for numerical computing in Python. It provides support for large, multi-dimensional arrays and various mathematical operations. Here are some common tasks:
Creating Arrays: Use NumPy's array function to create arrays from Python lists or directly generate arrays using functions like zeros, ones, or random.

1. Array Operations: Perform mathematical operations on arrays, such as element-wise arithmetic, array broadcasting, and linear algebra operations.
2. Data Cleaning: Use NumPy's functions to handle missing values (NaNs), perform data imputation, and remove outliers.
3. Array Slicing and Indexing: Access and manipulate specific elements or subsets of arrays using slicing and indexing techniques.
4. Array Reshaping and Transposing: Change the shape or dimensions of arrays using functions like reshape and transpose.

Pandas:
Pandas is a powerful library built on top of NumPy that provides high-level data structures and data analysis tools. It is particularly useful for working with structured data in tabular form. Here are some common tasks:
1. Reading Data: Use Pandas' read_csv, read_excel, or other I/O functions to load data from various file formats into a DataFrame.
2. Data Exploration: Perform initial data exploration by examining the DataFrame's structure, columns, and summary statistics using functions like head, info, describe, and shape.
3. Data Cleaning: Handle missing values, duplicate records, and outliers using functions like dropna, fillna, drop_duplicates, and clip.
4. Data Transformation: Apply data transformations like sorting, filtering, merging, and joining using functions like sort_values, query, merge, and concat.
5. Feature Engineering: Create new features from existing ones, perform feature scaling, and encode categorical variables using Pandas' functions and methods.
6. Data Aggregation: Group data based on specific criteria and calculate aggregations using functions like groupby, agg, and pivot_table.
7. Data Visualization: Use Pandas' integration with Matplotlib or Seaborn to visualize data, create plots, and gain insights.

Both NumPy and Pandas are highly compatible with other machine learning libraries in Python, such as scikit-learn and TensorFlow. By utilizing their functionalities, you can efficiently preprocess and manipulate data, making it suitable for machine learning tasks.

QUESTION 2
When working with large datasets in Python, libraries like Pandas and NumPy offer efficient ways to load, preprocess, clean, and transform the data. Here's an outline of the steps involved:

1. Loading the Dataset:
Use Pandas' read_csv or read_excel functions to load the dataset into a Pandas DataFrame. These functions provide options to specify data types, column names, delimiter settings, and handle missing values during loading.
import pandas as pd
# Load dataset from CSV file
df = pd.read_csv('dataset.csv')
2. Exploring the Data:
Start by examining the structure of the loaded DataFrame to understand its columns, data types, and any missing values. Use functions like head, info, describe, and shape to gain initial insights into the dataset.
# Print the first few rows
print(df.head())
# Get information about the DataFrame
print(df.info())
# Summary statistics of the dataset
print(df.describe())
3. Handling Missing Values:
Identify and handle missing values in the dataset. Depending on the data, you can choose to drop rows or columns with missing values using dropna, fill missing values using fillna with appropriate techniques (e.g., mean, median, or interpolation), or mark missing values with a specific placeholder.
# Drop rows with missing values
df = df.dropna()
# Fill missing values with mean
df = df.fillna(df.mean())
# Mark missing values with a placeholder
df = df.fillna('N/A')
4. Cleaning and Transforming Data:
Perform data cleaning tasks specific to your dataset. Some common techniques include:
· Handling Outliers: Use statistical methods or domain knowledge to identify and handle outliers, such as replacing outliers with a threshold value or removing them based on certain criteria.
· Encoding Categorical Variables: Convert categorical variables into numerical representations using techniques like one-hot encoding, label encoding, or binary encoding.
· Scaling and Normalizing: Scale numerical features to a similar range using techniques like min-max scaling or standardization (z-score normalization).
· Feature Engineering: Create new features by combining or transforming existing ones. For example, extract date features from timestamps or compute ratios between columns.
# Handling outliers
df = df[(df['Column'] > lower_threshold) & (df['Column'] < upper_threshold)]
# Encoding categorical variables
df = pd.get_dummies(df, columns=['Category'])
# Scaling numerical features
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df['Column'] = scaler.fit_transform(df['Column'].values.reshape(-1, 1))
# Feature engineering
df['NewFeature'] = df['Feature1'] + df['Feature2']
These are some common techniques for loading, preprocessing, cleaning, and transforming large datasets using Pandas and NumPy. Depending on the specific characteristics and requirements of your dataset, additional techniques and libraries may be needed.






















